---
title: "Practical Machine Learning"
author: "Ilias Dodoulas"
date: "03/10/2020"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Executive Summary

In this report, we use Machine Learning Algorithms in R to quantify how well people do a particular activity. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. By using data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants, we will apply a few machine learning algorithms to investigate how well an activity is performed.

## Loading the data and the nessecary packages

```{r Data_Packages}
library(caret)
library(rattle)

# Download files
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", destfile = "train.csv", method = "curl")
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", destfile = "test.csv", method = "curl")

# Open Files
training_set <- read.csv("train.csv")
testing_set <- read.csv("test.csv")
```

## Cleaning Data

The training set consists of 160 variables, most of which are empty. Therefore, we will need to do some data cleaning. For the purposes of these models, we will only use data related to the pitch, roll and yaw of the belt, arm, forearm and dumbell. The code that performs the abovementioned subsetting is given below:

```{r Data_Cleaning}
filter_string_belt <- "^roll_belt$|^pitch_belt$|^yaw_belt$|^roll_forearm$|^pitch_forearm$|^yaw_forearm$|^roll_arm$|^pitch_arm$|^yaw_arm$|^roll_dumbbell$|^pitch_dumbbell$|^yaw_dumbbell$|^classe$"
training_set_clean <- training_set[ , grepl(filter_string_belt, names(training_set))]
testing_set_clean <- testing_set[ , grepl(filter_string_belt, names(testing_set))]
training_set_clean$classe <- as.factor(training_set_clean$classe)
```

Still, the training data is a dataset of 19622 observations, something that makes the code extremely slow during the training phase. For the purposes of this exercise, and to be able to run the code in meaningful times, we will extract just a few observations from each classe subset and then combine them in a new training set. Of course, if execution time is not a problem, this step is not necessary.

```{r Data_Shrinking}
N <- 200
training_set_clean_A <- training_set_clean[training_set_clean$classe == "A", ]
training_set_clean_B <- training_set_clean[training_set_clean$classe == "B", ]
training_set_clean_C <- training_set_clean[training_set_clean$classe == "C", ]
training_set_clean_D <- training_set_clean[training_set_clean$classe == "D", ]
training_set_clean_E <- training_set_clean[training_set_clean$classe == "E", ]
training_set_clean <- rbind(training_set_clean_A[1:N,], training_set_clean_B[1:N,], training_set_clean_C[1:N,], training_set_clean_D[1:N,], training_set_clean_E[1:N,])
```

## Prediction with Trees

In our first approach, we will try to make a Tree Prediction Model. The code to do this and the respective chart are shown below:

```{r Tree Prediction}
modFit_Trees <- train(classe ~ ., method="rpart", data = training_set_clean)
print(modFit_Trees$finalModel)
fancyRpartPlot(modFit_Trees$finalModel)
prediction_Trees_train_set <- predict(modFit_Trees)
table(prediction_Trees_train_set, training_set_clean$classe)
prediction_Trees <- predict(modFit_Trees, newdata = testing_set_clean)
```

The code does not seem particularly successful, as we can see multiple data points misclassified even on the training set.

## Prediction with Random Forests

In the next approach, we will develop a random Forest Prediction algorithm as follows:

```{r Random Forest Prediction}
# Prediction with forests
modFit_Forest <-train(classe ~ ., method="rf", data = training_set_clean, prox=TRUE)
modFit_Forest
prediction_Forest_train_set <- predict(modFit_Forest)
table(prediction_Forest_train_set, training_set_clean$classe)
prediction_Forest <- predict(modFit_Trees, newdata = testing_set_clean)
```

This is a more successful prediction algorithm, with no misclassifications in the training set.

## Prediction with Boosting

As a next approach, we will try to build a Boosting Prediction Algorithm, as follows:

```{r Boosting Prediction}
# Prediction with Boosting
modFit_Boosting <-train(classe ~ ., method="gbm", data = training_set_clean, verbose=FALSE)
print(modFit_Boosting)
prediction_Boosting_train_set <- predict(modFit_Boosting)
table(prediction_Boosting_train_set, training_set_clean$classe)
prediction_Boosting <- predict(modFit_Boosting, newdata = testing_set_clean)
```

This algorithm also seems equally successful.

## Summary

In the final table below, we summarize the prediction output of the three tested algorithms on the test set:

```{r Summary}
# Summarize Results
Prediction_Table <- t(rbind(prediction_Trees, prediction_Forest, prediction_Boosting))
Prediction_Table
```